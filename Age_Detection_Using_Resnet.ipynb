{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2B/Qj2IfuND8VhLtXJN6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukul-mschauhan/Machine-Learning-Projects/blob/master/Age_Detection_Using_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AGe and Gender Detection Using UTKFace Dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/jangedoo/utkface-new\n",
        "\n",
        "UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc. The entire process is highlighted below:-\n",
        "\n",
        "\n",
        "* Imports\n",
        "* Dataset + DataLoader\n",
        "* Model definition\n",
        "* model = AgeGenderNet()\n",
        "* Optimizer = ...\n",
        "* Train / validate functions\n",
        "* Training loop"
      ],
      "metadata": {
        "id": "xVaez8jXuuvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iotNdvcrZsL",
        "outputId": "1a55d998-c537-436f-a8d5-3e29a28c211d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jangedoo/utkface-new?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331M/331M [00:01<00:00, 214MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw dataset path: /root/.cache/kagglehub/datasets/jangedoo/utkface-new/versions/1\n"
          ]
        }
      ],
      "source": [
        "!pip -q install kagglehub\n",
        "\n",
        "import os, glob, random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "\n",
        "print(\"Raw dataset path:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the actual folder that contains images"
      ],
      "metadata": {
        "id": "ZMJo8fpyrs7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "def find_image_dir(base_path):\n",
        "    best_dir = None\n",
        "    best_count = 0\n",
        "    for root, _, _ in os.walk(base_path):\n",
        "        jpg_count = len(glob.glob(os.path.join(root, \"*.jpg\")))\n",
        "        if jpg_count > best_count:\n",
        "            best_count = jpg_count\n",
        "            best_dir = root\n",
        "    return best_dir, best_count\n",
        "\n",
        "IMAGE_DIR, n_jpg = find_image_dir(path)\n",
        "\n",
        "print(\"Detected IMAGE_DIR:\", IMAGE_DIR)\n",
        "print(\"Number of .jpg images:\", n_jpg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjGcMTmNrnsa",
        "outputId": "4c98c588-e0d5-4187-d18b-1493eece9bbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected IMAGE_DIR: /root/.cache/kagglehub/datasets/jangedoo/utkface-new/versions/1/UTKFace\n",
            "Number of .jpg images: 23708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect valid images + split into train/val\n",
        "\n",
        "What this block does\n",
        "\n",
        "* glob(...) collects all .jpg files.\n",
        "\n",
        "* Filters out weird files by checking UTKFace naming convention:\n",
        "\n",
        "* filename format: age_gender_race_*.jpg\n",
        "\n",
        "so we require:\n",
        "\n",
        "* parts[0] is age (digit)\n",
        "\n",
        "* parts[1] is gender (digit)\n",
        "\n",
        "* Shuffles and splits 80/20 into training and validation paths.\n",
        "\n",
        "Why it‚Äôs needed\n",
        "\n",
        "* UTKFace datasets often contain a few bad files (broken names, non-image metadata, etc.).\n",
        "\n",
        "* Filtering prevents runtime crashes later."
      ],
      "metadata": {
        "id": "PlhG3Jqdr3FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "all_images = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
        "\n",
        "valid_images = []\n",
        "for p in all_images:\n",
        "    name = os.path.basename(p)\n",
        "    parts = name.split(\"_\")\n",
        "    if len(parts) >= 2 and parts[0].isdigit() and parts[1].isdigit():\n",
        "        valid_images.append(p)\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(valid_images)\n",
        "\n",
        "split = int(0.8 * len(valid_images))\n",
        "train_paths = valid_images[:split]\n",
        "val_paths   = valid_images[split:]\n",
        "\n",
        "print(\"Train:\", len(train_paths), \"Val:\", len(val_paths))\n",
        "assert len(train_paths) > 0 and len(val_paths) > 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D14jFDirwOZ",
        "outputId": "df6ae58c-8c5c-4056-a08c-efc7110748f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 18966 Val: 4742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforms: converting image ‚Üí tensor with consistent size\n",
        "\n",
        "What this block does\n",
        "\n",
        "* Resize: makes all images 224x224 so batching works.\n",
        "* Augmentation (train only): horizontal flip helps generalization.\n",
        "* ToTensor: converts PIL image to torch.Tensor in shape [C,H,W].\n",
        "* Normalize: matches ImageNet normalization because the backbone is pretrained on ImageNet.\n",
        "\n",
        "Why it‚Äôs needed\n",
        "\n",
        "* Without resizing: dataloader can‚Äôt stack images of different sizes.\n",
        "* Without ToTensor: you‚Äôll get collation errors because batches need tensors.\n",
        "* Without normalize: pretrained backbones perform worse (input distribution mismatch)."
      ],
      "metadata": {
        "id": "MCqtJstpsEqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "LNNOpfF9r46Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset class: read image + parse age/gender from filename\n",
        "\n",
        "What this block does\n",
        "\n",
        "* Implements a PyTorch Dataset:\n",
        "\n",
        "* __len__ tells how many samples.\n",
        "\n",
        "* __getitem__ returns one sample.\n",
        "\n",
        "Each sample returns:\n",
        "\n",
        "* img: tensor [3,224,224]\n",
        "\n",
        "* age: float tensor (e.g., 25.0)\n",
        "\n",
        "* gender: integer tensor (0 or 1)\n",
        "\n",
        "Why it‚Äôs safe\n",
        "\n",
        "* If filename parsing fails: it ‚Äúmoves on‚Äù to the next item.\n",
        "\n",
        "* If an image is corrupted: it also moves on.\n",
        "\n",
        "* This avoids dataloader crashes mid-epoch."
      ],
      "metadata": {
        "id": "DJsj1jjKsTXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class UTKFaceDatasetSafe(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        filename = os.path.basename(path)\n",
        "\n",
        "        parts = filename.split(\"_\")\n",
        "        if len(parts) < 2 or (not parts[0].isdigit()) or (not parts[1].isdigit()):\n",
        "            return self.__getitem__((idx + 1) % len(self.paths))\n",
        "\n",
        "        age = float(parts[0])\n",
        "        gender = int(parts[1])\n",
        "\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % len(self.paths))\n",
        "\n",
        "        if self.transform is None:\n",
        "            raise ValueError(\"Transform is None. Please pass train_tfms/val_tfms.\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        age = torch.tensor(age, dtype=torch.float32)\n",
        "        gender = torch.tensor(gender, dtype=torch.long)\n",
        "\n",
        "        return img, age, gender\n"
      ],
      "metadata": {
        "id": "DMlerAfHsIWm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader: batch + shuffle + workers\n",
        "\n",
        "What this block does\n",
        "\n",
        "* Wraps dataset into a batch generator.\n",
        "\n",
        "* shuffle=True for training so batches are random.\n",
        "\n",
        "* shuffle=False for validation so evaluation is stable.\n",
        "\n",
        "* num_workers=0 for maximum stability (especially in Colab).\n",
        "\n",
        "* pin_memory=True can speed up host‚ÜíGPU transfer."
      ],
      "metadata": {
        "id": "4Aampe7wsef9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = UTKFaceDatasetSafe(train_paths, transform=train_tfms)\n",
        "val_ds   = UTKFaceDatasetSafe(val_paths, transform=val_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "AiaBl1PSsRV9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "What this block does\n",
        "\n",
        "* Loads a pretrained ResNet18.\n",
        "\n",
        "* Removes its classifier (fc) by replacing with Identity.\n",
        "\n",
        "* The backbone outputs a feature vector f.\n",
        "\n",
        "Two task-specific heads:\n",
        "\n",
        "* Age regression head ‚Üí outputs 1 number per image (age).\n",
        "* Gender classification head ‚Üí outputs 2 logits (male/female).\n",
        "\n",
        "Why multi-task learning helps\n",
        "\n",
        "* The backbone learns face features useful for both tasks.\n",
        "* Gender task can regularize the representation and sometimes improves age too.\n",
        "\n",
        "Why gender head outputs ‚Äúlogits‚Äù\n",
        "* CrossEntropyLoss expects raw logits, not probabilities."
      ],
      "metadata": {
        "id": "13dwWREEsoBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "class AgeGenderNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        m = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "        feat_dim = m.fc.in_features\n",
        "        m.fc = nn.Identity()\n",
        "        self.backbone = m\n",
        "\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        age = self.age_head(f).squeeze(1)\n",
        "        gender_logits = self.gender_head(f)\n",
        "        return age, gender_logits\n"
      ],
      "metadata": {
        "id": "4mGeOpJiseFS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "Age loss: SmoothL1\n",
        "\n",
        "* Better than MSE when labels are noisy / outliers exist.\n",
        "\n",
        "* Penalizes big errors more gently than MSE.\n",
        "\n",
        "Gender loss: CrossEntropy\n",
        "\n",
        "* Standard for multi-class classification (binary included).\n",
        "\n",
        "Weighted sum\n",
        "\n",
        "``Final loss L = ùêøùëéùëîùëí + ùúÜ‚ãÖùêøùëîùëíùëõùëëùëíùëü``\n",
        "\n",
        "LAMBDA_GENDER balances tasks."
      ],
      "metadata": {
        "id": "n2xyj7Wbszl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_loss_fn = nn.SmoothL1Loss()\n",
        "gender_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "LAMBDA_GENDER = 1.0\n",
        "\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "model = AgeGenderNet().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeEynkPXso-9",
        "outputId": "3628490b-1c33-40fa-9a5a-1977dcf1d7e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "What happens in each iteration\n",
        "\n",
        "* Move batch to GPU/CPU.\n",
        "\n",
        "* Forward pass ‚Üí age + gender logits.\n",
        "\n",
        "* Compute two losses.\n",
        "\n",
        "* Combine them into a single loss.\n",
        "\n",
        "* Backprop + update weights.\n",
        "\n",
        "Track metrics:\n",
        "\n",
        "* MAE (years): average absolute age error.\n",
        "\n",
        "* Accuracy: gender correctness.\n",
        "\n",
        "Why ``detach()`` is used\n",
        "\n",
        "* Prevents metric computation from creating extra graph nodes (saves memory)."
      ],
      "metadata": {
        "id": "w6E7LMwwtnPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, total_age_mae, total_acc = 0.0, 0.0, 0.0\n",
        "\n",
        "    for x, age, gender in loader:\n",
        "        x = x.to(device)\n",
        "        age = age.to(device)\n",
        "        gender = gender.to(device)\n",
        "\n",
        "        pred_age, pred_gender_logits = model(x)\n",
        "\n",
        "        loss_age = age_loss_fn(pred_age, age)\n",
        "        loss_gender = gender_loss_fn(pred_gender_logits, gender)\n",
        "        loss = loss_age + LAMBDA_GENDER * loss_gender\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mae = (pred_age.detach() - age).abs().mean().item()\n",
        "        acc = (pred_gender_logits.detach().argmax(dim=1) == gender).float().mean().item()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_age_mae += mae * x.size(0)\n",
        "        total_acc += acc * x.size(0)\n",
        "\n",
        "    n = len(loader.dataset)\n",
        "    return total_loss / n, total_age_mae / n, total_acc / n"
      ],
      "metadata": {
        "id": "h8wZcXMss01m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Step"
      ],
      "metadata": {
        "id": "1n10l-qQty2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss, total_age_mae, total_acc = 0.0, 0.0, 0.0\n",
        "\n",
        "    for x, age, gender in loader:\n",
        "        x = x.to(device)\n",
        "        age = age.to(device)\n",
        "        gender = gender.to(device)\n",
        "\n",
        "        pred_age, pred_gender_logits = model(x)\n",
        "\n",
        "        loss_age = age_loss_fn(pred_age, age)\n",
        "        loss_gender = gender_loss_fn(pred_gender_logits, gender)\n",
        "        loss = loss_age + LAMBDA_GENDER * loss_gender\n",
        "\n",
        "        mae = (pred_age - age).abs().mean().item()\n",
        "        acc = (pred_gender_logits.argmax(dim=1) == gender).float().mean().item()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_age_mae += mae * x.size(0)\n",
        "        total_acc += acc * x.size(0)\n",
        "\n",
        "    n = len(loader.dataset)\n",
        "    return total_loss / n, total_age_mae / n, total_acc / n\n"
      ],
      "metadata": {
        "id": "ssL_FtbvtpTu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val = 1e9\n",
        "EPOCHS = 8\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_mae, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    va_loss, va_mae, va_acc = validate(model, val_loader, device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train: loss={tr_loss:.4f}, age_MAE={tr_mae:.2f}y, gender_acc={tr_acc:.3f} | \"\n",
        "        f\"val: loss={va_loss:.4f}, age_MAE={va_mae:.2f}y, gender_acc={va_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "    if va_loss < best_val:\n",
        "        best_val = va_loss\n",
        "        torch.save(model.state_dict(), \"best_age_gender.pt\")\n",
        "        print(\"  ‚úÖ saved best_age_gender.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq_TwLB3t0Pv",
        "outputId": "6cb288bb-a13a-4d8e-970c-22d357651909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train: loss=9.2638, age_MAE=9.42y, gender_acc=0.854 | val: loss=6.1858, age_MAE=6.39y, gender_acc=0.882\n",
            "  ‚úÖ saved best_age_gender.pt\n",
            "Epoch 02 | train: loss=5.8652, age_MAE=6.09y, gender_acc=0.890 | val: loss=5.3252, age_MAE=5.55y, gender_acc=0.899\n",
            "  ‚úÖ saved best_age_gender.pt\n",
            "Epoch 03 | train: loss=5.4347, age_MAE=5.69y, gender_acc=0.903 | val: loss=5.0699, age_MAE=5.29y, gender_acc=0.897\n",
            "  ‚úÖ saved best_age_gender.pt\n",
            "Epoch 04 | train: loss=5.0609, age_MAE=5.33y, gender_acc=0.913 | val: loss=5.3151, age_MAE=5.51y, gender_acc=0.883\n",
            "Epoch 05 | train: loss=4.8806, age_MAE=5.16y, gender_acc=0.920 | val: loss=5.3070, age_MAE=5.56y, gender_acc=0.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-4unSzDt1z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}